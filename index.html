<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, viewport-fit=cover">
    <title>SafeVision App</title>
    <link rel="manifest" href="manifest.json">
    <meta name="theme-color" content="#000000"> <link rel="apple-touch-icon" href="icon-192.png">

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

    <style>
        body {
            margin: 0;
            padding: 0;
            overflow: hidden; /* Hide scrollbars */
            background-color: black;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            color: white;
            font-family: sans-serif;
            flex-direction: column;
        }
        #webcam, #canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover; /* Ensures video covers screen without distortion */
            transform: scaleX(-1); /* Mirror camera feed for selfie view (common for front camera) */
            /* If using rear camera and want normal view, remove 'transform: scaleX(-1);' */
            display: none; /* Hidden by default until loaded */
        }
        #loadingMessage {
            position: absolute;
            z-index: 10;
            background: rgba(0, 0, 0, 0.7);
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            font-size: 1.2em;
        }
        #statusMessage {
            position: absolute;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0, 0, 0, 0.6);
            padding: 10px 20px;
            border-radius: 20px;
            font-size: 1.1em;
            color: limegreen; /* Green for normal status */
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
            max-width: 90%;
            z-index: 5;
        }
        .warning-text {
            color: red; /* Red for warnings */
        }
    </style>
</head>
<body>
    <video id="webcam" autoplay playsinline muted></video>
    <canvas id="canvas"></canvas>
    <div id="loadingMessage">Loading AI model... Please wait.</div>
    <div id="statusMessage" style="display: none;">App Ready</div> <script>
        const video = document.getElementById('webcam');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const loadingMessage = document.getElementById('loadingMessage');
        const statusMessage = document.getElementById('statusMessage');

        let model;
        let speechSynth = window.speechSynthesis;
        let isSpeaking = false;
        let lastSpeechTime = 0;
        const SPEECH_COOLDOWN = 3000; // 3 seconds cooldown between voice alerts

        // --- Performance Optimization Variables ---
        let frameCount = 0;
        const FRAMES_TO_SKIP = 2; // Process every 3rd frame (0, 1, 2 -> process 2)

        // --- Proximity Warning Threshold (Adjust this value) ---
        // Increase this number to make the "person is very close" warning trigger when the person is physically closer.
        // A value of 450-600 might be a good starting point for a typical phone orientation.
        // You'll need to experiment with this based on your camera and typical usage distance.
        const PROXIMITY_THRESHOLD_HEIGHT = 450; // Example: If person's bounding box height > 450px, trigger warning

        // --- PWA Service Worker Registration ---
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', () => {
                navigator.serviceWorker.register('service-worker.js')
                    .then(registration => {
                        console.log('Service Worker registered! Scope:', registration.scope);
                    })
                    .catch(err => {
                        console.log('Service Worker registration failed:', err);
                    });
            });
        }

        // --- Text-to-Speech Function ---
        // Handles speaking and cooldown to prevent rapid alerts
        function speak(text) {
            // Prevent new speech if already speaking or within cooldown period
            if (speechSynth.speaking || isSpeaking || (Date.now() - lastSpeechTime < SPEECH_COOLDOWN)) {
                return;
            }

            const utterance = new SpeechSynthesisUtterance(text);
            utterance.rate = 1.0; // Normal speed (1.0 is default, can be 0.1 to 10)
            utterance.pitch = 1.0; // Normal pitch (1.0 is default, can be 0 to 2)
            utterance.volume = 1.0; // Full volume (1.0 is default, can be 0 to 1)

            utterance.onstart = () => { isSpeaking = true; };
            utterance.onend = () => {
                isSpeaking = false;
                lastSpeechTime = Date.now();
            };
            utterance.onerror = (event) => {
                console.error('SpeechSynthesisUtterance.onerror', event);
                isSpeaking = false;
            };

            // Attempt to speak. On mobile, the very first speech might be blocked
            // if no prior user interaction (like a tap) has occurred.
            // However, removing the "click anywhere" prompt is the primary goal.
            speechSynth.speak(utterance);
        }

        // --- Camera Setup ---
        async function setupWebcam() {
            try {
                // Request camera stream. 'environment' for rear camera, 'user' for front.
                // Requesting ideal resolution to balance quality and performance.
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        facingMode: 'environment', // Use rear camera (usually better for object detection)
                        width: { ideal: 640 }, // Try to get 640px width
                        height: { ideal: 480 } // Try to get 480px height
                    }
                });
                video.srcObject = stream;

                return new Promise((resolve) => {
                    video.onloadedmetadata = () => {
                        // Set canvas dimensions to match video stream's actual dimensions
                        canvas.width = video.videoWidth;
                        canvas.height = video.videoHeight;
                        video.style.display = 'block'; // Show video once loaded
                        canvas.style.display = 'block'; // Show canvas once loaded
                        resolve(video);
                    };
                });
            } catch (error) {
                console.error("Error accessing camera:", error);
                loadingMessage.innerText = "Error: Camera access denied or not available. Please allow camera permissions in your device settings and refresh.";
                speak("Camera access denied. Please check permissions.");
                alert("Camera access denied or not available. Please allow camera permissions in your device settings and refresh the page.");
                return null; // Return null if camera setup fails
            }
        }

        // --- Load Model and Start Detection Loop ---
        async function loadModelAndStartDetection() {
            loadingMessage.innerText = "Loading AI model... This might take a moment depending on your internet speed.";
            statusMessage.style.display = 'none'; // Hide initial status until model is ready

            try {
                // Load the COCO-SSD model. Using 'lite_mobilenet_v2' base model for better mobile performance.
                model = await cocoSsd.load({base: 'lite_mobilenet_v2'});
                loadingMessage.style.display = 'none'; // Hide loading message once model is loaded
                statusMessage.style.display = 'block'; // Show status message area

                const webcamStream = await setupWebcam();
                if (webcamStream) {
                    video.play(); // Ensure video is playing
                    speak("App ready. Person detection active."); // Initial spoken message
                    detectFrame(); // Start the continuous detection loop
                }
            } catch (error) {
                console.error("Error loading model or starting camera:", error);
                loadingMessage.innerText = "Fatal Error: Could not load AI model or start camera. Check browser console for details.";
                speak("Fatal error. Could not load app.");
            }
        }

        // --- Main Detection Loop ---
        async function detectFrame() {
            if (video.readyState < 2) { // Ensure video is fully loaded and ready
                requestAnimationFrame(detectFrame);
                return;
            }

            // --- Frame Skipping Logic ---
            // Only process a frame every FRAMES_TO_SKIP + 1 iterations
            frameCount++;
            if (frameCount % (FRAMES_TO_SKIP + 1) !== 0) {
                requestAnimationFrame(detectFrame); // Request next frame without processing
                return;
            }

            // --- AI Model Inference and Drawing (within tf.tidy for automatic memory management) ---
            // tf.tidy ensures that all intermediate tensors created within this function
            // are disposed of automatically after the function completes, preventing memory leaks.
            tf.tidy(async () => {
                let predictions;
                try {
                    // Make sure model is loaded before attempting detection
                    if (model) {
                         predictions = await model.detect(video); // Perform detection on the current video frame
                    } else {
                         // Model not yet loaded or failed, skip detection for this frame
                         requestAnimationFrame(detectFrame);
                         return;
                    }
                } catch (e) {
                    console.error("Error during model detection:", e);
                    requestAnimationFrame(detectFrame); // Continue trying on next frame
                    return;
                }

                ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear previous drawings on canvas
                // Draw the current video frame onto the canvas. This is what the user sees.
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

                let personDetected = false;
                let isPersonClose = false;

                // Iterate over all detected objects
                predictions.forEach(prediction => {
                    // Check if the detected object is a 'person' and if the confidence score is high enough
                    if (prediction.class === 'person' && prediction.score > 0.6) { // 60% confidence threshold
                        personDetected = true;
                        const [x, y, width, height] = prediction.bbox; // Get bounding box coordinates and dimensions

                        // --- Drawing Bounding Box and Label ---
                        // Draw a green rectangle around the detected person
                        ctx.strokeStyle = 'lime';
                        ctx.lineWidth = 2;
                        ctx.strokeRect(x, y, width, height);

                        // Draw text label (e.g., "Person (95%)")
                        ctx.fillStyle = 'lime';
                        ctx.font = '16px sans-serif';
                        // Position text just above the box, or at the top of canvas if box is too high
                        ctx.fillText(`Person (${Math.round(prediction.score * 100)}%)`, x, y > 10 ? y - 5 : 10);

                        // --- Proximity Warning Logic ---
                        // If the height of the bounding box is greater than the defined threshold,
                        // it indicates the person is very close to the camera.
                        if (height > PROXIMITY_THRESHOLD_HEIGHT) {
                            isPersonClose = true;
                            // Change drawing style for the "very close" warning
                            ctx.strokeStyle = 'red';
                            ctx.lineWidth = 4;
                            ctx.strokeRect(x, y, width, height); // Draw a thicker red box
                            ctx.fillStyle = 'red';
                            ctx.fillText(`VERY CLOSE!`, x, y + height + 20); // Add "VERY CLOSE!" text below box
                        }
                    }
                });

                // --- Update Status Message and Play Voice Alerts ---
                if (isPersonClose) {
                    statusMessage.className = 'warning-text'; // Apply CSS class for red text
                    statusMessage.innerText = "WARNING: Person is very close!";
                    speak("Warning: Person very close.");
                } else if (personDetected) {
                    statusMessage.className = ''; // Remove warning class if not close
                    statusMessage.innerText = "Person detected.";
                } else {
                    statusMessage.className = ''; // No warning, no person
                    statusMessage.innerText = "No person detected.";
                }
            }); // End of tf.tidy() block â€“ all tensors created inside are automatically disposed.

            // Request the next animation frame to continue the detection loop
            requestAnimationFrame(detectFrame);
        }

        // --- Start the application automatically when the DOM is fully loaded ---
        document.addEventListener('DOMContentLoaded', loadModelAndStartDetection);
    </script>
</body>
</html>
