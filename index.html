<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <title>SafeVision</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <!-- Manifest for PWA features -->
    <link rel="manifest" href="manifest.json">
    <meta name="theme-color" content="#000000">
    <link rel="apple-touch-icon" href="icon-192.png">

    <style>
        body {
            margin: 0;
            font-family: sans-serif;
            background-color: #000;
            color: #fff;
            overflow: hidden;
            display: flex; /* Use flexbox to center video/canvas if needed */
            justify-content: center;
            align-items: center;
            height: 100vh; /* Full viewport height */
            width: 100vw;  /* Full viewport width */
        }
        /* Video and Canvas cover the entire screen */
        video, canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            object-fit: cover; /* Ensures video fills screen, may crop */
            z-index: 0;
        }
        /* Info overlay for status, weather, datetime, object detection */
        #info {
            position: absolute;
            top: 10px;
            left: 10px;
            text-align: left;
            background-color: rgba(0, 0, 0, 0.7);
            padding: 10px;
            border-radius: 8px;
            z-index: 2; /* Ensures it's above video/canvas */
        }
        /* Style for warning text, like "Person is very close" */
        .warning-text {
            color: red;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <video id="video" autoplay playsinline muted></video>
    <canvas id="canvas"></canvas>
    <div id="info">
        <div id="status">üîÑ Loading AI model...</div>
        <div id="weather">üå°Ô∏è Temp: 35¬∞C | üå¨Ô∏è Wind: 15 km/h | Sunny</div> <!-- Static weather display -->
        <div id="datetime"></div>
        <div id="object">üëÅÔ∏è Waiting for detection...</div>
        <div id="proximityStatus" style="color: limegreen;">üü¢ No person detected</div>
    </div>

    <!-- TensorFlow.js and COCO-SSD Model Libraries -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

    <script>
        // DOM Elements
        const video = document.getElementById("video");
        const canvas = document.getElementById("canvas");
        const ctx = canvas.getContext("2d");
        const statusEl = document.getElementById("status");
        const weatherEl = document.getElementById("weather");
        const datetimeEl = document.getElementById("datetime");
        const objectEl = document.getElementById("object");
        const proximityStatusEl = document.getElementById("proximityStatus");

        // AI Model and Speech Variables
        let model;
        let lastSpokenFullInfo = "";
        let speaking = false;
        const SPEECH_COOLDOWN_FULL_INFO = 50000; // 50 seconds for full info speech
        const SPEECH_COOLDOWN_WARNING = 3000; // 3 seconds for warning speech
        let lastFullInfoSpeechTime = 0;
        let lastWarningSpeechTime = 0;

        // Performance Optimization Variables
        let frameCount = 0;
        const FRAMES_TO_SKIP = 2; // Process every 3rd frame (0, 1, 2 -> process 2)

        // Proximity Warning Threshold (Adjust this value)
        const PROXIMITY_THRESHOLD_HEIGHT = 450;

        // --- PWA Service Worker Registration ---
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', () => {
                navigator.serviceWorker.register('service-worker.js')
                    .then(registration => {
                        console.log('Service Worker registered! Scope:', registration.scope);
                    })
                    .catch(err => {
                        console.log('Service Worker registration failed:', err);
                    });
            });
        }

        /**
         * Converts text to speech, with cooldowns to prevent rapid-fire alerts.
         * @param {string} text The text to speak.
         * @param {string} type 'fullInfo' for periodic updates, 'warning' for immediate alerts.
         */
        function speak(text, type = 'fullInfo') {
            console.log(`[TTS Debug] Attempting to speak (${type}): "${text}"`);

            let currentCooldown = 0;
            let lastSpeechTimeRef = 0;

            if (type === 'fullInfo') {
                currentCooldown = SPEECH_COOLDOWN_FULL_INFO;
                lastSpeechTimeRef = lastFullInfoSpeechTime;
            } else if (type === 'warning') {
                currentCooldown = SPEECH_COOLDOWN_WARNING;
                lastSpeechTimeRef = lastWarningSpeechTime;
            }

            if (speaking || (Date.now() - lastSpeechTimeRef < currentCooldown)) {
                console.log(`[TTS Debug] Speech (${type}) blocked due to cooldown or already speaking.`);
                return;
            }

            if (type === 'warning' && speechSynthesis.speaking) {
                speechSynthesis.cancel();
                console.log("[TTS Debug] Cancelled ongoing speech for warning.");
            } else if (type === 'fullInfo' && speechSynthesis.speaking) {
                console.log("[TTS Debug] Full info speech deferred, another speech is active.");
                return;
            }

            const voices = speechSynthesis.getVoices();
            if (voices.length === 0) {
                console.warn("[TTS Debug] No speech synthesis voices available on this device.");
                statusEl.textContent = "‚ùå Speech not available (no voices)";
                statusEl.style.color = 'red';
                return;
            } else {
                 console.log("[TTS Debug] Available voices:", voices.map(v => v.name));
            }


            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = "en-IN";

            const indianVoice = voices.find(voice => voice.lang === 'en-IN' || voice.lang === 'en-GB' || voice.name.includes('India'));
            if (indianVoice) {
                utterance.voice = indianVoice;
                console.log("[TTS Debug] Using voice:", indianVoice.name);
            } else {
                console.warn("[TTS Debug] No specific Indian/GB English voice found, using default.");
            }

            utterance.rate = 1.0;
            utterance.pitch = 1.0;
            utterance.volume = 1.0;

            utterance.onstart = () => { speaking = true; console.log("[TTS Debug] Speech started."); };
            utterance.onend = () => {
                speaking = false;
                if (type === 'fullInfo') {
                    lastFullInfoSpeechTime = Date.now();
                } else if (type === 'warning') {
                    lastWarningSpeechTime = Date.now();
                }
                console.log("[TTS Debug] Speech ended.");
            };
            utterance.onerror = (event) => {
                console.error('[TTS Debug] SpeechSynthesisUtterance.onerror:', event.error, event);
                speaking = false;
            };

            speechSynthesis.speak(utterance);
        }

        // --- Date and Time Update ---
        function updateDateTime() {
            const now = new Date();
            const date = now.toDateString();
            const time = now.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit', second: '2-digit' });
            datetimeEl.textContent = `üìÖ ${date} | ‚è∞ ${time}`;
        }
        setInterval(updateDateTime, 1000);
        updateDateTime();

        // --- Camera Setup ---
        async function setupCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        facingMode: { exact: "environment" },
                        width: { ideal: 640 },
                        height: { ideal: 480 }
                    },
                    audio: false
                });
                video.srcObject = stream;
                return new Promise((resolve) => {
                    video.onloadedmetadata = () => {
                        video.play();
                        resizeCanvas();
                        resolve(video);
                    };
                });
            } catch (error) {
                console.error("Error accessing camera:", error);
                statusEl.textContent = "‚ùå Camera access denied or not available. Please allow permissions and refresh.";
                statusEl.style.color = 'red';
                return null;
            }
        }

        // --- Canvas Resizing ---
        function resizeCanvas() {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
        }

        // --- Main Object Detection Loop ---
        async function detectFrame() {
            if (video.readyState < 2) {
                requestAnimationFrame(detectFrame);
                return;
            }

            frameCount++;
            if (frameCount % (FRAMES_TO_SKIP + 1) !== 0) {
                requestAnimationFrame(detectFrame);
                return;
            }

            let predictions;
            try {
                if (model) {
                    predictions = await model.detect(video);
                } else {
                    requestAnimationFrame(detectFrame);
                    return;
                }
            } catch (e) {
                console.error("Error during model detection (async part):", e);
                requestAnimationFrame(detectFrame);
                return;
            }

            tf.tidy(() => {
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                // No need to draw video on canvas if object-fit:cover is used for video element itself
                // ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

                let personDetected = false;
                let isPersonClose = false;
                let detectedObjectsText = "üëÅÔ∏è No objects detected.";

                predictions.forEach(pred => {
                    if (pred.score > 0.6) {
                        const [x, y, width, height] = pred.bbox;
                        ctx.strokeStyle = "#00FF00";
                        ctx.lineWidth = 2;
                        ctx.strokeRect(x, y, width, height);

                        ctx.fillStyle = "#00FF00";
                        ctx.font = '16px sans-serif';
                        ctx.fillText(`${pred.class} (${Math.round(pred.score * 100)}%)`, x, y > 10 ? y - 5 : 10);

                        detectedObjectsText = `üëÅÔ∏è I see a ${pred.class}`;

                        if (pred.class === 'person') {
                            personDetected = true;
                            if (height > PROXIMITY_THRESHOLD_HEIGHT) {
                                isPersonClose = true;
                                ctx.strokeStyle = 'red';
                                ctx.lineWidth = 4;
                                ctx.strokeRect(x, y, width, height);
                                ctx.fillStyle = 'red';
                                ctx.fillText(`VERY CLOSE!`, x, y + height + 20);
                            }
                        }
                    }
                });

                objectEl.textContent = detectedObjectsText;

                if (isPersonClose) {
                    proximityStatusEl.className = 'warning-text';
                    proximityStatusEl.textContent = "üî¥ WARNING: Person very close!";
                    speak("Warning: Person very close.", 'warning');
                } else if (personDetected) {
                    proximityStatusEl.className = '';
                    proximityStatusEl.textContent = "üü¢ Person detected";
                } else {
                    proximityStatusEl.className = '';
                    proximityStatusEl.textContent = "üü¢ No person detected";
                }
            });

            requestAnimationFrame(detectFrame);
        }

        // --- Initial App Load Function ---
        async function main() {
            statusEl.textContent = "üîÑ Loading AI model...";
            // Delay for voices to load, and to potentially satisfy autoplay policy with a slight wait
            await new Promise(resolve => setTimeout(resolve, 500)); // Wait 0.5 seconds

            // Explicitly call getVoices AFTER a short delay.
            // Some browsers require a microtask to complete before voices are fully registered.
            speechSynthesis.onvoiceschanged = () => {
                console.log("[TTS Debug] Voices loaded event fired. Voices:", speechSynthesis.getVoices().map(v => v.name));
            };
            // Call getVoices immediately to populate the list if already ready
            speechSynthesis.getVoices();


            await setupCamera();
            if (!video.srcObject) {
                console.error("Camera setup failed, aborting main execution.");
                return;
            }

            model = await cocoSsd.load({base: 'lite_mobilenet_v2'});
            statusEl.textContent = "‚úÖ AI model loaded!";

            // Initial full information speech after model loads
            speakFullInformation(); // Call once initially
            setInterval(speakFullInformation, SPEECH_COOLDOWN_FULL_INFO); // Then periodically

            detectFrame();
        }

        // Function to combine all info for periodic speech
        function speakFullInformation() {
            const combinedText = `
                Status: ${statusEl.textContent}.
                Weather: ${weatherEl.textContent}.
                Date and Time: ${datetimeEl.textContent}.
                Object detection: ${objectEl.textContent}.
                Proximity status: ${proximityStatusEl.textContent}.
            `.replace(/\s+/g, ' ').trim();

            if (combinedText !== lastSpokenFullInfo) {
                 speak(combinedText, 'fullInfo');
                 lastSpokenFullInfo = combinedText;
            }
        }

        document.addEventListener('DOMContentLoaded', main);
    </script>
</body>
</html>
