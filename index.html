<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <title>SafeVision</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <link rel="manifest" href="manifest.json">
    <meta name="theme-color" content="#000000">
    <link rel="apple-touch-icon" href="icon-192.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        html, body {
            margin: 0;
            padding: 0;
            width: 100vw;
            height: 100vh;
            overflow: hidden;
        }
        body {
            font-family: sans-serif;
            background-color: #000;
            color: #fff;
            width: 100vw;
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            position: relative;
        }
        video, canvas {
            position: absolute;
            top: 0; left: 0;
            width: 100vw; height: 100vh;
            object-fit: cover;
            z-index: 0;
        }
        #info {
            position: absolute;
            top: 10px; left: 10px;
            background-color: rgba(0, 0, 0, 0.7);
            padding: 10px;
            border-radius: 8px;
            z-index: 2;
            font-size: 0.9em;
        }
        .warning-text { color: red; font-weight: bold; }
        #speechRecognitionStatus {
            position: absolute;
            top: 10px; right: 10px;
            background-color: rgba(0,0,0,0.7);
            padding: 8px 12px;
            border-radius: 8px;
            color: yellow;
            font-size: 0.9em;
            display: flex;
            align-items: center;
            gap: 5px;
            z-index: 3;
        }
        #speechRecognitionStatus .fas {
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0% { transform: scale(1); opacity: 1; }
            50% { transform: scale(1.1); opacity: 0.8; }
            100% { transform: scale(1); opacity: 1; }
        }
    </style>
</head>
<body>
    <video id="video" autoplay playsinline muted></video>
    <canvas id="canvas"></canvas>
    <div id="info">
        <div id="status">üîÑ Loading AI model...</div>
        <div id="weather">üå°Ô∏è Temp: 35¬∞C | üå¨Ô∏è Wind: 15 km/h | Sunny</div> 
        <div id="datetime"></div>
        <div id="object">üëÅÔ∏è Waiting for detection...</div>
        <div id="proximityStatus" style="color: limegreen;">üü¢ No person detected</div>
    </div>
    <div id="speechRecognitionStatus">
        <i class="fas fa-microphone"></i> Listening for commands...
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
    <script>
        // --- DOM Elements ---
        const video = document.getElementById("video");
        const canvas = document.getElementById("canvas");
        const ctx = canvas.getContext("2d");
        const statusEl = document.getElementById("status");
        const weatherEl = document.getElementById("weather");
        const datetimeEl = document.getElementById("datetime");
        const objectEl = document.getElementById("object");
        const proximityStatusEl = document.getElementById("proximityStatus");
        const speechRecognitionStatusEl = document.getElementById("speechRecognitionStatus");

        // --- Variables ---
        let model, recognition, fullInfoIntervalId, currentRecognitionState = 'IDLE';
        let speaking = false, isMuted = false;
        let lastSpokenFullInfo = "";
        const SPEECH_COOLDOWN_FULL_INFO = 60000, SPEECH_COOLDOWN_WARNING = 3000;
        let lastFullInfoSpeechTime = 0, lastWarningSpeechTime = 0;

        // --- Service Worker ---
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', () => {
                navigator.serviceWorker.register('service-worker.js');
            });
        }

        // --- Date & Time ---
        function updateDateTime() {
            const now = new Date();
            datetimeEl.textContent = `üìÖ ${now.toDateString()} | ‚è∞ ${now.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit', second: '2-digit' })}`;
        }
        setInterval(updateDateTime, 1000); updateDateTime();

        // --- Camera Setup ---
        async function setupCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode: 'environment', width: { ideal: 640 }, height: { ideal: 480 } },
                    audio: false
                video.videoWidth;
            canvas.height = video.videoHeight;
        }

        // --- Object Detection ---
        let frameCount = 0, FRAMES_TO_SKIP = 2;
        const PROXIMITY_THRESHOLD_HEIGHT = 450;
        async function detectFrame() {
            if (video.readyState < 2) { requestAnimationFrame(detectFrame); return; }
            frameCount++;
            if (frameCount % (FRAMES_TO_SKIP + 1) !== 0) { requestAnimationFrame(detectFrame); return; }
            let predictions;
            try {
                if (model) predictions = await model.detect(video);
                else { requestAnimationFrame(detectFrame); return; }
            } catch { requestAnimationFrame(detectFrame); return; }
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            let personDetected = false, detectedObjectsText = "üëÅÔ∏è No objects detected.";
            predictions.forEach(pred => {
                if (pred.score > 0.6) {
                    const [x, y, width, height] = pred.bbox;
                    ctx.strokeStyle = "#00FF00";
                    ctx.lineWidth = 2;
                    ctx.strokeRect(x, y, width, height);
                    ctx.fillStyle = "#00FF00";
                    ctx.font = '16px sans-serif';
                    ctx.fillText(`${pred.class} (${Math.round(pred.score * 100)}%)`, x, y > 10 ? y - 5 : 10);
                    detectedObjectsText = `üëÅÔ∏è I see a ${pred.class}`;
                    if (pred.class === 'person') {
                        personDetected = true;
                        if (height > PROXIMITY_THRESHOLD_HEIGHT) {
                            ctx.strokeStyle = 'red';
                            ctx.lineWidth = 4;
                            ctx.strokeRect(x, y, width, height);
                            ctx.fillStyle = 'red';
                            ctx.fillText(`VERY CLOSE!`, x, y + height + 20);
                            speak("Warning: Person very close.", 'warning');
                        }
                    }
                }
            });
            objectEl.textContent = detectedObjectsText;
            proximityStatusEl.textContent = personDetected ? "üü¢ Person detected" : "üü¢ No person detected";
            requestAnimationFrame(detectFrame);
        }

        // --- Speech Synthesis ---
        async function speak(text, type = 'fullInfo') {
            if (isMuted && type !== 'emergency') return;
            let cooldown = (type === 'warning') ? SPEECH_COOLDOWN_WARNING : SPEECH_COOLDOWN_FULL_INFO;
            let lastSpeech = (type === 'warning') ? lastWarningSpeechTime : lastFullInfoSpeechTime;
            if (speaking || (Date.now() - lastSpeech < cooldown)) return;
            if (type === 'emergency' && speechSynthesis.speaking) speechSynthesis.cancel();
            const voices = speechSynthesis.getVoices();
            if (!voices.length) return;
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = "en-IN";
            const indianVoice = voices.find(voice => voice.lang === 'en-IN' || voice.lang === 'en-GB' || voice.name.includes('India'));
            if (indianVoice) utterance.voice = indianVoice;
            utterance.rate = 1.0; utterance.pitch = 1.0; utterance.volume = 1.0;
            return new Promise(resolve => {
                utterance.onstart = () => { speaking = true; };
                utterance.onend = () => {
                    speaking = false;
                    if (type === 'fullInfo') lastFullInfoSpeechTime = Date.now();
                    else if (type === 'warning') lastWarningSpeechTime = Date.now();
                    resolve();
                };
                utterance.onerror = () => { speaking = false; resolve(); };
                speechSynthesis.speak(utterance);
            });
        }
        function stripEmojis(text) {
            return text.replace(/(\u00a9|\u00ae|[\u2000-\u3300]|\ud83c[\ud000-\udfff]|\ud83d[\ud000-\udfff]|\ud83e[\ud000-\udfff])/g, '');
        }
        function speakFullInformation() {
            if (currentRecognitionState === 'IDLE') {
                const combinedText = `
                    Status: ${stripEmojis(statusEl.textContent)}.
                    Weather: ${stripEmojis(weatherEl.textContent)}.
                    Date and Time: ${stripEmojis(datetimeEl.textContent)}.
                    Object detection: ${stripEmojis(objectEl.textContent)}.
                    Proximity status: ${stripEmojis(proximityStatusEl.textContent)}.
                `.replace(/\s+/g, ' ').trim();
                if (combinedText !== lastSpokenFullInfo) {
                    speak(combinedText, 'fullInfo');
                    lastSpokenFullInfo = combinedText;
                }
            }
        }

        // --- Speech Recognition ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        function setupSpeechRecognition() {
            if (!SpeechRecognition) {
                speechRecognitionStatusEl.innerHTML = "‚ùå Voice commands not supported.";
                speechRecognitionStatusEl.style.color = 'red';
                return;
            }
            recognition = new SpeechRecognition();
            recognition.continuous = true; 
            recognition.interimResults = true;
            recognition.lang = 'en-IN';
            recognition.onresult = async (event) => {
                const results = event.results;
                for (let i = event.resultIndex; i < results.length; i++) {
                    const transcript = results[i][0].transcript.toLowerCase().trim();
                    const isFinal = results[i].isFinal;
                    if (!isFinal) {
                        speechRecognitionStatusEl.innerHTML = `<i class="fas fa-microphone"></i> Listening: ${transcript}...`;
                        continue;
                    }
                    if (isFinal && currentRecognitionState === 'IDLE') {
                        if (transcript.includes("mute")) {
                            isMuted = true;
                            await speak("Muted.", 'emergency');
                            speechRecognitionStatusEl.innerHTML = `<i class="fas fa-microphone"></i> Muted | Listening for commands...`;
                            continue;
                        }
                        if (transcript.includes("unmute")) {
                            isMuted = false;
                            await speak("Unmuted.", 'emergency');
                            speechRecognitionStatusEl.innerHTML = `<i class="fas fa-microphone"></i> Listening for commands...`;
                            continue;
                        }
                        // EMERGENCY!
                        if (transcript.includes("emergency")) {
                            await speak("Calling emergency services at 112.", 'emergency');
                            return;
                        }
                    }
                }
            };
            recognition.onend = () => {
                if (recognition && recognition.continuous) {
                    try { recognition.start(); } catch {}
                }
            };
        }
        function startContinuousRecognition() {
            if (recognition && currentRecognitionState === 'IDLE') {
                try {
                    recognition.continuous = true;
                    recognition.interimResults = true;
                    recognition.start();
                    speechRecognitionStatusEl.innerHTML = '<i class="fas fa-microphone"></i> Listening for commands...';
                } catch {}
            }
        }

        // --- Main App ---
        async function main() {
            statusEl.textContent = "üîÑ Loading AI model... SafeVision";
            await new Promise(resolve => setTimeout(resolve, 500));
            await setupCamera();
            if (!video.srcObject) return;
            model = await cocoSsd.load({base: 'lite_mobilenet_v2'});
            statusEl.textContent = "‚úÖ AI model loaded!";
            setupSpeechRecognition();
            startContinuousRecognition();
            await speak("Welcome to SafeVision.", 'fullInfo'); 
            speakFullInformation();
            fullInfoIntervalId = setInterval(speakFullInformation, SPEECH_COOLDOWN_FULL_INFO);
            detectFrame();
        }
        document.addEventListener('DOMContentLoaded', main);
    </script>
</body>
</html>